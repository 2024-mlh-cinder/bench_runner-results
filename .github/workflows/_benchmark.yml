# Generated file: !!! DO NOT EDIT !!!
---
env:
  PYPERFORMANCE_HASH: f7f36509e2e81e9a20cfeadddd6608f2378ff26c
  PYSTON_BENCHMARKS_HASH: ee8adbd7846ec67d1a8a362e6a5e876df372431d
name: _benchmark
on:
  workflow_call:
    inputs:
      fork:
        description: Fork of cpython to benchmark
        type: string
      reponame:
        description: Name of cpython repo to benchmark
        type: string
      ref:
        description: Branch, tag or (full) SHA commit to benchmark
        type: string
      benchmarks:
        description: Benchmarks to run (comma-separated; empty runs all benchmarks)
        type: string
      pgo:
        description: Build with PGO
        type: boolean
      dry_run:
        description: 'Dry run: Do not commit to the repo'
        type: boolean
      force:
        description: Rerun and replace results if commit already exists
        type: boolean
      perf:
        description: Collect Linux perf profiling data (Linux only)
        type: boolean
      tier2:
        description: Enable the Tier 2 interpreter
        type: boolean
      jit:
        description: Enable the JIT
        type: boolean

  workflow_dispatch:
    inputs:
      fork:
        description: Fork of cpython to benchmark
        type: string
        default: facebookincubator
      reponame:
        description: Name of cpython repo to benchmark
        type: string
        default: cinder
      ref:
        description: Branch, tag or (full) SHA commit to benchmark
        type: string
        default: main
      benchmarks:
        description: Benchmarks to run (comma-separated; empty runs all benchmarks)
        type: string
      pgo:
        description: Build with PGO
        type: boolean
      dry_run:
        description: 'Dry run: Do not commit to the repo'
        type: boolean
      force:
        description: Rerun and replace results if commit already exists
        type: boolean
      perf:
        description: Collect Linux perf profiling data (Linux only)
        type: boolean
      tier2:
        description: Enable the Tier 2 interpreter
        type: boolean
      jit:
        description: Enable the JIT
        type: boolean

jobs:
  setup:
    runs-on: [self-hosted]

    steps:
    - name: Install System Dependencies
      run: |
        sudo apt update
        sudo apt install -y python3 build-essential ccache gdb lcov pkg-config \
        libbz2-dev libffi-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \
        libncurses5-dev libreadline-dev libsqlite3-dev libssl-dev \
        lzma lzma-dev tk-dev uuid-dev zlib1g-dev linux-tools-generic

    - name: Enable ccache
      run: |
        sudo /usr/sbin/update-ccache-symlinks
        echo 'export PATH="/usr/lib/ccache:$PATH"' | tee -a ~/.bashrc

    - name: Cache ccache files
      uses: actions/cache@v3
      with:
        path: ~/.ccache
        key: ${{ runner.os }}-ccache-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-ccache-

  benchmark-linux-x86_64-linux:
    needs: setup
    runs-on: [self-hosted]
    timeout-minutes: 1440

    steps:
    - name: Checkout benchmarking
      uses: actions/checkout@v3
    - name: Setup system Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: pip
    - name: Checkout CPython
      uses: actions/checkout@v3
      with:
        repository: ${{ inputs.fork }}/${{ inputs.reponame }}
        path: ${{ inputs.reponame }}
        ref: ${{ inputs.ref }}
        fetch-depth: 50
    - name: Install dependencies from PyPI
      run: |
        python -m venv venv
        venv/bin/python -m pip install -r requirements.txt
    - name: Should we run?
      if: ${{ always() }}
      id: should_run
      run: |
        venv/bin/python -m bench_runner should_run ${{ inputs.force }} ${{ inputs.fork }} ${{ inputs.ref }} linux-x86_64-aws false ${{ inputs.tier2 }} ${{ inputs.jit }}>> $GITHUB_OUTPUT
    - name: Checkout python-macrobenchmarks
      uses: actions/checkout@v3
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      with:
        repository: pyston/python-macrobenchmarks
        path: pyston-benchmarks
        ref: ${{ env.PYSTON_BENCHMARKS_HASH }}
    - name: Checkout pyperformance
      uses: actions/checkout@v3
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      with:
        repository: python/pyperformance
        path: pyperformance
        ref: ${{ env.PYPERFORMANCE_HASH }}
    - name: Enable Tier 2
      if: ${{ inputs.tier2 }}
      run: |
        echo "PYTHON_UOPS=1" >> "$GITHUB_ENV"
    - name: Build Python
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      run: |
        cd ${{ inputs.reponame }}
        ./configure ${{ inputs.pgo == true && '--enable-optimizations --with-lto=yes' || '' }} ${{ inputs.jit == true && '--enable-experimental-jit' || '' }}
        make
    - name: Install pyperformance
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      run: |
        venv/bin/python -m pip install --no-binary :all: ./pyperformance
    - name: Tune system
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      run: |
        sudo LD_LIBRARY_PATH=$LD_LIBRARY_PATH venv/bin/python -m pyperf system tune
    - name: Tune for (Linux) perf
      if: ${{ steps.should_run.outputs.should_run != 'false' && inputs.perf }}
      run: |
        sudo bash -c "echo 100000 > /proc/sys/kernel/perf_event_max_sample_rate"
    - name: Running pyperformance
      if: ${{ steps.should_run.outputs.should_run != 'false' }}
      run: |
        rm -rf ~/.debug/*
        venv/bin/python -m bench_runner run_benchmarks ${{ inputs.perf && 'perf' || 'benchmark' }} ${{ inputs.reponame }}/python ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.benchmarks || 'all' }} --run_id ${{ github.run_id }} ${{ inputs.tier2 == true && '--flag PYTHON_UOPS' || '' }} ${{ inputs.jit == true && '--flag JIT' || '' }}
    - name: Upload benchmark artifacts
      if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.perf }}
      uses: actions/upload-artifact@v3
      with:
        name: benchmark
        path: |
          benchmark.json
    - name: Upload perf artifacts
      if: ${{ steps.should_run.outputs.should_run != 'false' && inputs.perf }}
      uses: actions/upload-artifact@v3
      with:
        name: perf
        path: |
          profiling/results
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run &&
        !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run &&
        !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
